%%%%%%%% ICML 2022 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[nohyperref]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{enumitem,pifont}
\usepackage{subcaption}
%\usepackage{calrsfs}
%\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}
%\usepackage{mathrsfs}
%\usepackage[math-style=TeX, bold-style=TeX]{unicode-math}
%\setmathfont{XITS Math}
%\setmathfont[range={\mathscr,\mathbfscr,\mathcal,\mathbfcal}]{Cambria Math}
%\def\Lcs#1{\texttt{\textbackslash#1}}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2022} with \usepackage[nohyperref]{icml2022} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2022}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2022}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
%\DeclareUnicodeCharacter{2212}{-}
% Defining mycomment
\newcounter{mycomment}
\newcommand{\mycomment}[2][]{%
% initials of the author (optional) + note in the margin
\refstepcounter{mycomment}{%\setstretch{0.7}
\todo[color={red!100!green!33}]{%
\textbf{Comment [\uppercase{#1}\themycomment]:}~#2}%
}}



\makeatletter
\if@todonotes@disabled
\newcommand{\hlfix}[2]{#1}
\else
\newcommand{\hlfix}[2]{\texthl{#1}\todo{#2}}
\fi
\makeatother



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% More space in the margins
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\paperwidth=\dimexpr \paperwidth + 6cm\relax
%\oddsidemargin=\dimexpr\oddsidemargin + 3cm\relax
%\evensidemargin=\dimexpr\evensidemargin + 3cm\relax
%\marginparwidth=\dimexpr \marginparwidth + 3cm\relax
%\setlength{\marginparwidth}{ 3cm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Graphics path 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\graphicspath{{figures/}} 
% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2022}

\begin{document}

\twocolumn[
\icmltitle{Population-Based Stochastic Pruning: How Noise can reveal good sparse Networks}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2022
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Luis Alfredo Avenda単o-Mu単oz}{yyy}
\icmlauthor{Nabi Omidvar}{yyy}
\icmlauthor{Netta Cohen}{yyy}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
% %\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Engineering and Physical Science School, School of Computing, University of Leeds, Leeds, United Kingdom}
%\icmlaffiliation{comp}{Company Name, Location, Country}
%\icmlaffiliation{sch}{School of Computing}


\icmlcorrespondingauthor{Luis Alfredo Avenda単o-Mu単oz}{sclaam@leeds.ac.uk}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning,Neural Networks, Pruning, , ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

Neural network pruning has shown that many weights can be safely removed from modern neural networks without compromising accuracy.
Since then, a variety of pruning techniques have been used, each claiming to be superior to the preceding. Many state-of-the-art (SOTA) strategies used today rely on sophisticated pruning procedures using significance scores, backpropagation feedback, or pruning rules based on heuristics, among other things.
Here, we propose a pruning algorithm that does not use fine-tuning for accuracy recovery and is simple to implement. We call this algorithm Population-Based Stochastic Pruning.
We tested our algorithm on ResNet18 with CIFAR10 obtaining 75\% accuracy for a pruning rate of 90\%.


\end{abstract}

\section{Introduction}
\input{introduction.tex}

\section{Related work}
%\subsection{Outline}
% \begin{itemize}
%     \item There has been many research about neural network pruning and sparsity:

% \item         Some of the use second order
%         \cite{singhWoodFisherEfficientSecondOrder2020,
%         yuHessianAwarePruningOptimal2022}

%     \item  Some of them before training \todo{cite snip grasp and synflow} but they
%         cannot reach the same performance as post training pruning
%         \cite{franklePruningNeuralNetworks2021}.
%     \item There is another branch that is dynamical sparse training, where the mask is constantly changing  during the course of training \todo{Cite Riging the lottery, Evolutionary sparse training ,DEEP R, and many others I have in my zotero}    
    
% \end{itemize}
\input{related_work.tex}
\section{Neural Network Pruning}



The primary premise of neural network pruning is that, since neural networks are frequently over-parameterized, it is possible to achieve the same performance with a substantially smaller network \cite{reedPruningAlgorithmsaSurvey1993}, while improving generalisation.
To achieve this, the objective is to find a sparse network that maintains
the performance of the over-parameterized network. Then we follow the
formulation of neural network pruning as an optimisation problem of
\cite{leeSNIPSINGLESHOTNETWORK2018}. Given a dataset $\mathcal{D}=\{(x_i,y_i)\} _{i=1}^{n}$
and a desired pruning level $\kappa$ (that is, the number of zero weights),
the optimisation problem for neural network pruning can be stated as follows:
\begin{equation}
     \centering
    \begin{aligned}
          \underset{\mathbf{w,m}}{\min}\quad&  \frac{1}{n} \sum_{i=1}^{n}
          \ell(F(\mathbf{m}\odot \textbf{w};x_i),y_i)     \\
    \label{eq:pruning_optim_problem}
           s.t.\quad & \mathbf{w}\in \mathbb{R}^d\\
           \quad & \mathbf{m}\in \{0,1\}^d\quad \|\mathbf{m}\|_0 \geq \kappa
       \end{aligned}
\end{equation}
           where $\ell(\cdot)$ is the loss function, $\odot$ is the
           Hadamard product, $F(\mathbf{m}\odot \textbf{w};x)$ is the output
           of the neural network given the set of weights and masks, $d$ is the total number of
           parameters and $\|\cdot\|_0$ is the norm $L_0$.
Most after-training pruning methods reveal a static mask $m$ and fine-tune the remaining weights $\tilde{\mathbf{w}}=\mathbf{m}
\odot\mathbf{w}$ to minimise the accuracy drop $\epsilon= ACC(\mathbf{w^*})
-ACC(\mathbf{\tilde{w}})$. This operation is expensive, since gradients need to be calculated for
the whole training dataset, and in general they are calculated in a dense
manner (even when weights are 0 they are still used in the backward pass). 



In this work, we propose an iterative gradient-free population-based algorithm
that minimises $\epsilon$. Our approach is more efficient than fine-tuning and
is very simple to implement.
\section{Experiments}
We use a ResNet18 \cite{heDeepResidualLearning2015} trained for 200 epoch with
SGD, initial learning rate of 0.1, cosine annealing schedule with $T_{max}=200$ and batch size of 128.
First of all, we shed light on the fact that in the one-shot no fine-tuning regime,
additive noise with magnitude pruning can reveal good performing subnetworks.
On the basis of this, we propose a population-based algorithm that recovers accuracy
without fine-tuning.

\input{results.tex}

%\section{Conclusions}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  REFERENCES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\nocite{*}
\bibliography{Pruning}
\bibliographystyle{icml2022}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\input{appendix.tex}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022. 
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
 Codrina Lauth. Alex Smola contributed to the algorithmic style files.
