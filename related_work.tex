While there are a variety of approaches to compressing neural networks, such
as the novel design of efficient architectures
\cite{tanEfficientNetRethinkingModel2019,
howardMobileNetsEfficientConvolutional2017,
sandlerMobileNetV2InvertedResiduals2019}, optimisation in a random subspace of
the
parameter space \cite{liMeasuringIntrinsicDimension2022}, and training of dynamic sparse networks 
\cite{mocanuScalableTrainingArtificial2018a,bellecDeepRewiringTraining2018,
evciRiggingLotteryMaking2020,dettmersSparseNetworksScratch2019} , in this work we will focus on neural network pruning.
Neural network pruning can be grouped into \textit{After training pruning}
and \textit{Pre-training } 
% Also in this section we mention \textit{dynamical sparse training} were the
% network's
% connectivity  is also updated during training to obtain a good performing sparse network.

\textbf{Pruning After Training:} These methods prune a pretrained network.
Each method has a different importance score to detect unimportant weights.
The most simple is the magnitude of a weight
\cite{hanLearningBothWeights2015a,hanDeepCompressionCompressing2016a} where the
weights with the lowest magnitude are deemed as
 unnecessary. However, this method can remove important low-magnitude weights.
 Some methods use the first-order Taylor expansion coefficients of the loss function (i.e. gradient information) as importance scores for pruning. \cite{karninsimpleprocedurepruning1990a,mozerSkeletonizationTechniqueTrimming1988}.
 Hessian-based methods measure the importance of weights as the effect they
 have in the second-order approximation of the final loss function 
 \cite{hassibiSecondOrderDerivatives1992,lecunOptimalBrainDamage1989,
 hassibiOptimalBrainSurgeon1993,singhWoodFisherEfficientSecondOrder2020}.
 Many of these methods require a fine-tuning phase to mitigate the accuracy
 drop.




\textbf{Pre-Training Pruning:} This family of algorithms prunes a
network before training on randomly initialised weights. SynFlow
\cite{tanakaPruningNeuralNetworks2020a} uses a measure based on the path norm
\cite{neyshaburPathSGDPathNormalizedOptimization2015} that the authors call
synaptic flow, which they prove avoids layer collapse, this method does not use
data for pruning. Gradient Signal Preservation
(GrasP) \cite{wangPickingWinningTickets2020} prunes the randomly initialised network so that the
gradient flow signal is preserved throughout the network.
Lastly, Single-Shot Network Pruning Based on Connection Sensitivity
\cite{leeSNIPSINGLESHOTNETWORK2018} add parameters that represent
connections (but not weight) between neurons and uses the approximate
gradient of the loss function with respect to the new parameters as the
saliency score for pruning. 

% \textbf{Dynamical Sparse Training:} In this category methods instead of
% pruning once and leaving the mask static, they alter the mask during the
% course of training which results in similar or better performance than the
% dense counterparts.
% This idea was first used in Sparse
% Evolutionary Training (SET) \cite{mocanuScalableTrainingArtificial2018a} were
% they initialized the network in a sparse manner and after
% each epoch  a fraction of the weights closest to zero is pruned and the same
% number of weights is grown back randomly.
% Sparse Networks from Scratch (SNFS) \cite{dettmersSparseNetworksScratch2019}
% regrowth method is based on the momentum of each weight, reallocating the weights from less efficient layers and redistribute them to weight-efficient layers.
% Rigging the Lottery (RigL)  \cite{evciRiggingLotteryMaking2020}h strategy is to use the
% "would be" gradients of the pruned parameters as the indicator for
% incorporating new weights to the network. Deep R
% \cite{bellecDeepRewiringTraining2018} uses a probabilistic framework for
% regrowing and pruning connections on the network alongside to adding a random
% walk to SGD in parameter space.
